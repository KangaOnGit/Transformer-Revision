{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwr8ZK02eglbL0W1SgGsKt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KangaOnGit/Transformer-Revision/blob/main/Transformer_Architect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qd0eFgPTzqaE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_length, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.word_emb = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embed_dim\n",
        "        )\n",
        "        self.pos_emb = nn.Embedding(\n",
        "            num_embeddings=max_length,\n",
        "            embedding_dim=embed_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, seq_len = x.size()\n",
        "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
        "        output1 = self.word_emb(x)\n",
        "        output2 = self.pos_emb(positions)\n",
        "        output = output1 + output2\n",
        "        return output"
      ],
      "metadata": {
        "id": "0hY33ONtz0Yi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "        )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        attn_output, _ = self.attn(query, key, value)\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        out_1 = self.layernorm_1(query + attn_output)\n",
        "        ffn_output = self.ffn(out_1)\n",
        "        ffn_output = self.dropout_2(ffn_output)\n",
        "        out_2 = self.layernorm_2(out_1 + ffn_output)\n",
        "        return out_2\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionEmbedding(\n",
        "            src_vocab_size, embed_dim, max_length, device\n",
        "        )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerEncoderBlock(\n",
        "                    embed_dim, num_heads, ff_dim, dropout\n",
        "                ) for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, output, output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Ve0R8Z3Ez1TP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "        )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_3 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "        self.dropout_3 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output, _ = self.attn(x, x, x, attn_mask=tgt_mask)\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        out_1 = self.layernorm_1(x + attn_output)\n",
        "\n",
        "        attn_output, _ = self.cross_attn(\n",
        "            out_1, enc_output, enc_output, attn_mask=src_mask\n",
        "        )\n",
        "        attn_output = self.dropout_2(attn_output)\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out_2)\n",
        "        ffn_output = self.dropout_3(ffn_output)\n",
        "        out_3 = self.layernorm_3(out_2 + ffn_output)\n",
        "        return out_3\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, tgt_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionEmbedding(\n",
        "            tgt_vocab_size, embed_dim, max_length, device\n",
        "        )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerDecoderBlock(\n",
        "                    embed_dim, num_heads, ff_dim, dropout\n",
        "                ) for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        output = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, enc_output, src_mask, tgt_mask)\n",
        "        return output"
      ],
      "metadata": {
        "id": "iCX9MRLYz3Af"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.encoder = TransformerEncoder(\n",
        "            src_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim\n",
        "        )\n",
        "        self.decoder = TransformerDecoder(\n",
        "            tgt_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_dim, tgt_vocab_size)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_seq_len = src.shape[1]\n",
        "        tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "        src_mask = torch.zeros(\n",
        "            (src_seq_len, src_seq_len),\n",
        "            device=self.device).type(torch.bool)\n",
        "\n",
        "        tgt_mask = (torch.triu(torch.ones(\n",
        "            (tgt_seq_len, tgt_seq_len),\n",
        "            device=self.device)\n",
        "        ) == 1).transpose(0, 1)\n",
        "        tgt_mask = tgt_mask.float().masked_fill(\n",
        "            tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        enc_output = self.encoder(src)\n",
        "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "HRrvb2vgz5UK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "src_vocab_size = 1000\n",
        "tgt_vocab_size = 2000\n",
        "embed_dim = 200\n",
        "max_length = 100\n",
        "num_layers = 2\n",
        "num_heads = 4\n",
        "ff_dim = 256\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size, tgt_vocab_size,\n",
        "    embed_dim, max_length, num_layers, num_heads, ff_dim\n",
        ")\n",
        "\n",
        "src = torch.randint(\n",
        "    high=2,\n",
        "    size=(batch_size, max_length),\n",
        "    dtype=torch.int64\n",
        ")\n",
        "\n",
        "tgt = torch.randint(\n",
        "    high=2,\n",
        "    size=(batch_size, max_length),\n",
        "    dtype=torch.int64\n",
        ")\n",
        "\n",
        "prediction = model(src, tgt)\n",
        "prediction.shape\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJkPqTQPz5_-",
        "outputId": "f8f5e715-07e6-4231-eaba-d9516c53faba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-4.3922e-01, -7.5503e-01, -3.5588e-01,  ..., -9.9065e-02,\n",
            "           6.9077e-01, -5.3283e-01],\n",
            "         [ 1.0917e+00, -1.8979e-01, -1.7541e-02,  ...,  5.3405e-02,\n",
            "          -1.2061e-01, -1.4928e+00],\n",
            "         [ 5.8930e-01, -7.0639e-01,  2.3616e-01,  ..., -4.3380e-01,\n",
            "          -4.1785e-01, -3.4352e-01],\n",
            "         ...,\n",
            "         [ 2.7770e-02, -6.1925e-01,  1.3589e-01,  ..., -6.7973e-01,\n",
            "           5.2055e-01, -1.2567e+00],\n",
            "         [-4.6187e-01, -5.8572e-01,  4.1146e-01,  ...,  2.0589e-01,\n",
            "           2.7820e-01, -4.9549e-01],\n",
            "         [-6.0090e-01, -1.4780e-01, -5.4957e-02,  ...,  5.4267e-01,\n",
            "           6.6884e-01, -2.1523e-01]],\n",
            "\n",
            "        [[ 8.5565e-02, -1.9640e-01, -3.2118e-01,  ...,  7.6286e-02,\n",
            "           9.8561e-03, -6.1187e-01],\n",
            "         [ 4.7757e-01, -8.4081e-01, -2.7106e-01,  ...,  1.0332e-01,\n",
            "           9.9491e-02, -2.0066e+00],\n",
            "         [-5.9636e-04, -1.1425e+00, -9.0679e-02,  ..., -5.2936e-01,\n",
            "          -3.4671e-01, -8.8680e-01],\n",
            "         ...,\n",
            "         [ 1.1159e-01, -7.7076e-01, -1.3854e-01,  ..., -6.7892e-01,\n",
            "           5.9212e-01, -1.2846e+00],\n",
            "         [-4.1297e-01, -4.2913e-01,  1.3431e-01,  ...,  4.8217e-01,\n",
            "          -9.9632e-02, -5.6641e-01],\n",
            "         [-7.5333e-01,  1.6414e-01,  1.6661e-02,  ...,  5.2841e-01,\n",
            "           4.0446e-01, -2.9533e-01]],\n",
            "\n",
            "        [[ 1.7879e-01, -1.9825e-02, -3.7328e-01,  ...,  1.9035e-01,\n",
            "          -6.8469e-04, -5.9736e-02],\n",
            "         [ 3.3513e-01, -7.4427e-01, -2.4350e-01,  ...,  1.9026e-01,\n",
            "           1.7964e-01, -1.7848e+00],\n",
            "         [-9.6705e-03, -1.4929e+00,  8.5060e-02,  ..., -3.9970e-01,\n",
            "          -1.7620e-01, -6.5984e-01],\n",
            "         ...,\n",
            "         [ 7.1010e-01, -2.7534e-02, -4.1622e-01,  ..., -4.2577e-01,\n",
            "           1.7273e-01, -7.1365e-01],\n",
            "         [-1.5493e-01, -7.3238e-01,  5.0244e-01,  ...,  3.9782e-01,\n",
            "           2.4552e-01, -6.1310e-01],\n",
            "         [ 2.6055e-01,  5.4846e-01, -3.3205e-01,  ...,  4.9343e-01,\n",
            "           6.7687e-02,  1.8904e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-3.6076e-01, -7.5217e-01, -5.5718e-01,  ..., -7.9555e-02,\n",
            "           5.0387e-01, -5.8358e-01],\n",
            "         [ 6.7461e-01, -7.4777e-01, -1.8607e-01,  ...,  1.2374e-01,\n",
            "           6.2909e-01, -1.6313e+00],\n",
            "         [ 7.5719e-01, -6.5881e-01,  4.7473e-02,  ..., -4.0950e-01,\n",
            "          -3.9295e-01,  1.2458e-02],\n",
            "         ...,\n",
            "         [ 1.4428e-01, -5.4140e-01, -2.0717e-01,  ..., -7.9735e-01,\n",
            "           5.4721e-01, -1.4988e+00],\n",
            "         [-1.9710e-01, -6.5762e-01,  2.4424e-01,  ...,  4.4511e-01,\n",
            "           2.5020e-01, -5.1022e-01],\n",
            "         [-7.2252e-01,  4.0772e-02,  8.7642e-02,  ...,  5.1134e-01,\n",
            "           4.3952e-01, -2.9200e-01]],\n",
            "\n",
            "        [[ 1.7454e-01, -4.1466e-01, -2.4485e-01,  ...,  2.7650e-01,\n",
            "          -2.3735e-01, -4.5004e-01],\n",
            "         [ 7.8087e-01,  3.1965e-02, -9.3514e-02,  ...,  4.8132e-01,\n",
            "          -2.6038e-01, -1.3240e+00],\n",
            "         [ 5.4921e-01, -5.0390e-01, -6.1368e-02,  ..., -3.2653e-01,\n",
            "          -1.8040e-01, -3.6954e-01],\n",
            "         ...,\n",
            "         [ 5.2819e-01,  6.8222e-02, -3.5922e-01,  ..., -7.0123e-01,\n",
            "           2.5252e-01, -8.4201e-01],\n",
            "         [ 5.6026e-01, -9.2384e-02,  4.4786e-01,  ...,  5.0337e-01,\n",
            "          -1.2157e-01,  1.2289e-01],\n",
            "         [ 2.7050e-01,  4.2553e-01, -1.9869e-01,  ...,  5.2526e-01,\n",
            "           5.5256e-02,  2.6017e-02]],\n",
            "\n",
            "        [[ 4.1459e-01, -3.8482e-01, -7.0904e-01,  ..., -6.5314e-02,\n",
            "          -6.1283e-02, -4.0812e-01],\n",
            "         [ 3.0564e-01, -6.1464e-01, -1.9570e-01,  ...,  3.0326e-02,\n",
            "           2.6309e-01, -1.9131e+00],\n",
            "         [-1.7718e-02, -1.0948e+00, -1.9611e-01,  ..., -4.1998e-01,\n",
            "          -2.3947e-01, -6.2164e-01],\n",
            "         ...,\n",
            "         [-1.1280e-01, -6.6684e-01,  3.1439e-02,  ..., -7.4013e-01,\n",
            "           6.2386e-01, -1.3032e+00],\n",
            "         [-2.6858e-01, -4.9531e-01,  4.1000e-01,  ...,  2.9820e-01,\n",
            "           3.2408e-02, -5.2674e-01],\n",
            "         [-4.2688e-01, -1.3130e-02,  2.5771e-01,  ...,  5.3895e-01,\n",
            "           2.0580e-01, -1.3510e-01]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}